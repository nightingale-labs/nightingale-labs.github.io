[
    {   
        "id": "gcb",
        "title": "Evaluating convergence between two data visualization literacy assessments",
        "tagline": "Data visualizations are only useful if people know how to read them.",
        "imgSrc": "/research_posters/gcb.png",
        "imgAlt": "Analyzing response consistency between humans and models",
        "abstract": "We want to help everyone acquire the visualization literacy skills they need to understand the various plots, charts, and graphs they might encounter. To do that, we need to know how to measure data visualization literacy. There are a few different tests out there, including VLAT (Lee, Kim, & Kwon, 2016) and GGR (Galesic & Garcia-Retamero, 2011). The goal of our study was to understand if these two tests measure the same skills. To find out, we recruited U.S. adults (N=1,113) to complete both tests in one sitting. We found that people who did well on one test also often did well on the other one (r=0.56). So maybe both tests do measure the same or similar underlying skills? And if so, what are those skills? It would be convenient if these skills corresponded to things like 'the ability to read a bar plot' or 'the ability to find extrema.' But our analyses revealed that it's not so simple. Just knowing what kind of graph is being shown (e.g., bar plot) or knowing what kind of question is being asked (e.g., 'Can you find the maximum value?') is not enough to explain why a person got some questions right and other questions wrong. We used exploratory factor analysis to identify alternative ways of carving up 'data visualization literacy' and the latent factors we discovered did a much better job of accounting for the pattern of mistakes that people made. But interpreting what these factors are remains challenging, and we think that is mostly due to limitations of these tests (i.e., too many variables confounded and not enough questions). The upshot is that more work needs to be done to develop unified, reliable, and interpretable measures of data visualization literacy.",
        "ref": "Brockbank, E., Verma, A., Lloyd, H., Huey, H., Padilla, L., and Fan, J. (2025). Evaluating convergence between two data visualization literacy assessments.",
        "publication_venue": "Cognitive Research: Principles and Implications.",
        "pdf_link": "https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-025-00622-9"
    },

    {
        "id":"vt-fusion",
        "title": "Measuring and predicting variation in the difficulty of questions about data visualizations",
        "tagline":"What makes some data visualizations more difficult to understand than others?",
        "imgSrc": "/research_posters/vt-fusion.png",
        "imgAlt": "tmp",
        "abstract": "Some visualizations are more challenging to make sense of than others, regardless of how 'literate' you are. Why is that? As a first step, we collected five influential data visualization literacy tests, containing a total of 230 test items (Wainer, 1980; Galesic & Garcia-Retamero, 2011; Boy, Rensink, Bertini, & Fekete, 2014; Lee, Kim, & Kwon, 2016; Ge, Cui, & Kay, 2023). These tests had been designed independently of one another and with different goals in mind: one focused on assessing graph literacy in elementary school children (Wainer, 1980) while another focused on the ability of adults to make sense of deliberately misleading plots (Ge, Cui, & Kay, 2023). We recruited U.S. adults (N=503), each of whom answered a subset of questions from all five tests, yielding precise estimates of the relative difficulty of all items. Consistent with our other recent work (Brockbank et al., 2025), we found that knowing what kind of graph was used (e.g., bar graph, scatter plot, dot plot) and what kind of question was asked (e.g., retrieve value, arithmetic computation) explained some of the reliable variation in item-level difficulty, but most of the variation remained unexplained by these factors. We envision this dataset being used as a way to rigorously test the ability of any cognitive model to make quantitatively accurate predictions about the difficulty of different tasks involving data visualizations",
        "ref": "Verma, A., and Fan, J. E. (2025). Measuring and predicting variation in the difficulty of questions about data visualizations.",
        "publication_venue": "Proceedings of the 47th Annual Meeting of the Cognitive Science Society.",
        "pdf_link": "https://arxiv.org/pdf/2505.08031"
    },

    {   
        "id": "chart6",
        "title": "CHART-6: Human-centered evaluation of data visualization understanding in vision-language models",
        "tagline": "How well do current AI systems understand data visualizations?",
        "imgSrc": "/research_posters/chart6.png",
        "imgAlt": "Analyzing response consistency between humans and models",
        "abstract":"Although there are lots of AI systems that can perform sophisticated tasks involving images, it is still not clear how well they can read data visualizations. That's in part because previous work tested AI systems using different measures from those developed for people. Our goal was to close this gap by combining multiple tests that had been designed by people for people and administering them to several vision-language models under tightly controlled settings. The models we tested often failed to provide interpretable responses, even when we tried to give them the benefit of the doubt (i.e., multiple tries; a lenient scoring procedure). Moreover, these models performed worse than the people we had recruited to take the same tests, and they tended to make different kinds of mistakes than people did. In sum, our study suggests that more work is needed to build AI systems that read data visualizations the way that people do, and it is important to use human-centered and rigorous evaluation methods so we do not draw premature conclusions regarding the capabilities of AI systems on visual reasoning tasks.",
        "ref": "Verma, A., Mukherjee, K., Potts, C., Kreiss, E., & Fan, J. E. (2025). CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models.",
        "publication_venue": "arXiv preprint arXiv:2505.17202.",
        "pdf_link": "https://arxiv.org/pdf/2505.17202"

    },
    {   
        "id": "fugu",
        "title": "Diagnosing bottlenecks in data visualization understanding by vision-language models",
        "tagline": "What explains why current AI systems struggle to understand data visualizations?",
        "imgSrc": "/research_posters/fugu.png",
        "imgAlt": "tmp",
        "abstract":"Our work has found that many otherwise impressive AI systems still struggle to understand data visualizations. This poses challenges for cognitive scientists who want to understand the fundamental perceptual and cognitive mechanisms that make it possible to convey abstract ideas in visual form, including as data visualizations. We want to understand why these systems struggle on some tasks, but succeed on others. Towards answering this question, we studied the internal operations that LLaMA-3.2 11B — an influential AI system — uses when posed with questions about data visualizations. We wanted to determine whether failures happen because this model is unable to properly encode visual information, because information gets lost when transferring between the vision and language components, or because the language component cannot process the information correctly. We tracked how information flows through different parts of the model and found that the model often incorrectly identifies the coordinates of individual data points, and these early mistakes cascade into wrong final answers. However, when we provided the model with correct coordinates, its performance improved dramatically. This suggests that once the model has accurate coordinate information, it can perform the necessary mathematical reasoning quite well. Importantly, even when the model gave wrong answers, we could extract the correct coordinates from its internal visual processing components. This finding suggests that the vision-model component can extract the relevant information, but something goes wrong when handing that information off to the language-model component. More work is needed to understand how to overcome this 'vision-language bottleneck' to improve the multimodal reasoning capabilities of AI systems.",
        "ref": "Tartaglini, A. R., Grant, S., Wurgaft, D., Potts, C., & Fan, J. E. (2025). Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models.",
        "publication_venue": "arXiv preprint arXiv:2510.21740.",
        "pdf_link": "https://arxiv.org/pdf/2510.21740"
    },
    {
        "id": "dsp-solds",
        "title": "Linking student psychological orientation, engagement, and learning in college-level introductory data science",
        "tagline": "What learning environments help people acquire data literacy?",
        "imgSrc": "/research_posters/solds.png",
        "imgAlt": "tmp",
        "abstract": "Data literacy helps people make sense of a complex and unpredictable world. Obviously, people are not born knowing how to draw well calibrated inferences from quantitative evidence. Instead, they (are meant to) acquire these skills in school. Without these skills, people are more vulnerable to misinformation, mis-estimate risks, and make poorer decisions. But the current status quo in statistics and data science education falls far short of what is needed in a world full of data and still rife with uncertainty. Our goal is to conduct studies that interrogate the broad array of factors that predict student success in college-level statistics and data science courses. We are partnering with CourseKata, which develops a digital learning platform, to track over a thousand students across dozens of institutions as they develop their statistical reasoning skills over the course of an academic term. These fine-grained, longitudinal student data will help link the 'micro-scale' decisions that students make, moment-to-moment — such as whether to persist with one activity or move on — with the 'macro-scale' outcomes that impact what knowledge and skills they take away from these courses.",
        "ref": "Zheng, K., Brockbank, E., Schwartz, S. T., Bryan, C., Dweck, C., and Fan, J. E. (2025). Linking student psychological orientation, engagement, and learning in college-level introductory data science.",
        "publication_venue": "Proceedings of the 47th Annual Meeting of the Cognitive Science Society.",
        "pdf_link": "https://cogtoolslab.github.io/pdf/zheng_cogsci_2025.pdf"
    }
]